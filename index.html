<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation">
    <meta name="keywords" content="Multi-Sensory, Robotic Manipulation, Multi-Stage">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation</title>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true } });
    </script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/logo.png"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .card {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 10px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 7px;
            background-color: #f1f1f1;
        }

        .card:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }

        .card2 {
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
            /* transition: 0.3s; */
            /* width: 300px; */
            border-radius: 7px;
            /* 圆角的大小 */
            /* margin: 50px; */
            padding: 5px;
            background-color: #f1f1f1;
        }

        .card2:hover {
            box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
        }
    </style>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://gewu-lab.github.io/">
                            GeWu Lab@RUC
                        </a>

                    </div>
                </div>
            </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation</h1>
                        <h3 class="title is-5 publication-title">Computer Vision and Pattern Recognition
                            (CVPR) 2025</h3>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Henghui Du<sup>1,3</sup>,</span>
                            <span class="author-block">
                                Guangyao Li<sup>2</sup>,</span>
                            <span class="author-block">
                                Chang Zhou<sup>3</sup>,</span>
                            <span class="author-block">
                                Chunjie Zhang<sup>3</sup>,</span>
                            <span class="author-block">
                                Alan Zhao<sup>3</sup>,</span>
                            <span class="author-block">
                                Di Hu<sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Renmin
                                University of China &nbsp; &nbsp; <sup>2</sup>Tsinghua University</span><br />
                            <span class="author-block"><sup>3</sup>
                                AI Technology Center, Online Video Business Unit, Tencent PCG</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://openreview.net/pdf?id=jt7HPDmsHv"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block">
                                    <a href="https://rick-xu315.github.io/ICASSP23_Sup.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Supplementary</span>
                                    </a>
                                </span> -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2503.13068" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/GeWu-Lab/Crab"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/ahsgdxhs/AVUIE" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database main-icon"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://huggingface.co/ahsgdxhs/Crab"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-dice-d6"></i>
                                        </span>
                                        <span>Checkpoint</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
    </section>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Overview</h2>
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <!-- <embed src="./static/images/all.pdf" type="application/pdf" width="100" height="100"> -->
                <img src='./static/images/intro.png' width="100%" height="100%">
                <br><br>
                <div class="content has-text-justified">
                    <p>
                        We present Crab, a unified audio-visual scene understanding model with explicit cooperation, which can complete various
                        audio-visual tasks. It is trained on an instruction-tuning dataset with explicit reasoning process, which clarifies the cooperative relationship
                        among tasks. Furthermore, to alleviate the interference caused by the learning process of complex audiovisual data and facilitate concrete
                        cooperation, an interaction-aware LoRA structure is designed to enable the model focus on different aspects of data interaction.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">AV-UIE: Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process</h2>
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <img src='./static/images/dataset.png' width="100%" height="100%">
                <br><br>
                <div class="content has-text-justified">
                    <p> AV-UIE is an audio-visual unified instruction-tuning dataset with explicit reasoning process, which is primarily an 
                        augmentation of existing audio-visual task datasets. Through above dataset construction process, 
                        a detailed reasoning process can be obtained, which contains rich information in terms of temporal
                        and spatial, conductive to temporal and spatial localization tasks.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Crab Model</h2>
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <img src='./static/images/model.png' width="100%" height="100%">
                <br><br>
                <div class="content has-text-justified">
                    <p> Crab model mainly consists of two parts: unified audio-visual
                        interface, which consists of three multimodal branches, and a large language model with interaction-aware LoRA structure. The audio
                        branch and visual branch process audio and video inputs respectively, while the segmentation branch is responsible for outputting the
                        segmentation mask. The model is trained on our AV-UIE dataset, which clarifies the cooperation relationship among tasks, as marked
                        by different colors on the right side of the figure. Content of same color in different tasks can help model establish cooperative relationship
                        among tasks. Furthermore, to alleviate the interference caused by the learning process of complex audiovisual data, we design an
                        interaction-aware LoRA structure to facilitate concrete cooperation.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h2 class="title is-3 has-text-centered">Experiments</h2>
                <h2 class="title is-4">Comparision with general models </h2>
                <img src='./static/images/result1.png'
                    style="margin-top: 5px; margin-bottom: 10px; display: block; margin-left: auto; margin-right: auto;"
                    width="90%">
                <div class="content has-text-justified">
<!--                     <p>We incorporate data from <span style="color: rgb(61, 127, 207);">GelSight</span>, <span
                            style="color: rgb(222, 156, 8);">GelSlim</span>, <span
                            style="color: rgb(0, 180, 26);">DIGIT</span> and <span
                            style="color: rgb(151, 81, 203);">GelSight Mini</span> into
                        the training of AnyTouch to obtain four different models, and compare them across four
                        downstream tasks. We observe performance improvements across the three unseen datasets, with
                        greater enhancements for unseen sensors than seen sensors. This suggests that the knowledge from
                        the data of GelSlim, DIGIT, and GelSight Mini can transfer to the GelSight and other sensors.
                    </p> -->
<!--                     <p>
                        The comparison results with other general models on all type of tasks. MS3 and AVSS are two subtasks of AVS-Bench. Seen is a
                        subtask of the Ref-AVS test set. The X-InstructBLIP’s performance on AVQA is zero-shot. ✓ indicates the model has ability to complete
                        this type of task, but no evaluation is provided in their paper. ✗ indicates the model does not have the corresponding ability.
                    </p> -->

                </div>
                <!-- <p>The results are shown in Table 1:</p> -->
                <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.pdf">
            </video> -->
                <!-- <embed src="./static/images/method.pdf" type="application/pdf"> -->
                <!-- <h2 class="subtitle has-text-justified">
                    $\dagger$ indicates MMCosine is applied. Combined with MMCosine, most of the fusion methods gain
                    considerable improvement for datasets of various scales, domains, and label amount.
                </h2> -->

                <h2 class="title is-4">Comparison with specialized model on AVE & AVVP dataset</h2>
                <img src='./static/images/result3.png'
                    style="margin-top: 5px; margin-bottom: 10px; display: block; margin-left: auto; margin-right: auto;"
                    width="90%">
<!--                 <div class="container is-max-desktop">
                    <div class="columns is-centered" style="display: flex; align-items: center; margin-bottom: 0px;">
                        <div class="column">
                            <img src='./static/images/result3.png'>
                        </div>

                        <div class="column">
                            <img src='./static/images/result4.png'>
                        </div>
                    </div>
                </div> -->
                <div class="content has-text-justified">
<!--                     <p>
                        The comparison results with specialized models on temporal localization task.
                    </p> -->
                </div>


                <h2 class="title is-4">Comparison with specialized model on MUSIC-AVQA dataset</h2>
<!--                 <img src='./static/images/result2.png' style=" margin-bottom: 5px;"> -->
                 <img src='./static/images/result2.png'
                    style="margin-top: 5px; margin-bottom: 10px; display: block; margin-left: auto; margin-right: auto;"
                    width="90%">
                <div class="content has-text-justified">
<!--                     <p> 
                        The comparison results with specialized models on MUSIC-AVQA test set.
                    </p> -->
                </div>

                
                <h2 class="title is-4">Visualized results</h2>
               <img src='./static/images/result5.png'
                    style="margin-top: 5px; margin-bottom: 10px; display: block; margin-left: auto; margin-right: auto;"
                    width="90%">
                <div class="content has-text-justified">
<!--                     <p>To test the dynamic perception capability of our method in real-world object manipulation tasks,
                        we conduct experiments on a real-world task: fine-grained pouring. The robot arm must rely
                        entirely on tactile feedback to pour out 60g of small beads from a cylinder that initially
                        contains 100g of beads. We conduct 10 real-world test runs and record the mean error. The results
                        demonstrate the importance of learning unified multi-sensor representations from both static
                        and dynamic perspectives for completing various tasks including real-world tasks.</p> -->
                </div>
            </div>
        </div>
    </section>



    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{du2025crab,
  title={Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation},
  author={Du, Henghui and Li, Guangyao and Zhou, Chang and Zhang, Chunjie and Zhao, Alan and Hu, Di},
  journal={arXiv preprint arXiv:2503.13068},
  year={2025}
}</code></pre>
    </div>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://openreview.net/pdf?id=jt7HPDmsHv">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/GeWu-Lab/Crab" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            Thanks to <a href="https://nerfies.github.io/">Nerfies</a> for providing the
                            template of this page.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>
